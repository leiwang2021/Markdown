# 推荐系统论文

## Item2Vec

- 主要做法是把item视为word，用户的行为序列视为一个集合，item间的共现为正样本，并按照item的频率分布进行负样本采样，缺点是相似度的计算还只是利用到了item共现信息，1).忽略了user行为序列信息; 2).没有建模用户对不同item的喜欢程度高低。

- Item2vec中**把用户浏览的商品集合等价于word2vec中的****word的序列，即句子**（忽略了商品序列空间信息spatial information） 。出现在同一个集合的商品对视为 positive。对于集合![[公式]](https://www.zhihu.com/equation?tex=w_%7B1%7D%2C+w_%7B2%7D%2C+...%2Cw_%7BK%7D)目标函数：

  ![](/home/leiwang/Markdown/推荐系统/v2-fbd91efdf95148c289b53c39de93756f_hd.png)

  - 最终，利用SGD方法学习的目标函数max，得到每个商品的embedding representation，商品之间两两计算cosine相似度即为商品的相似度。 
  - 如果对比与word2vec去理解，一个客户浏览记录（item1-item2-item3-item4-item5）作为一个完整句子（word1-word2-word3-word4-word5）。word2vec中，比如CBoW，给定word1, word2, word4, word5,那么word3即为正样本。通过negative samping方法，采样的其他词语作为负样本。 对比到item的例子中，理解在item1，item2， item4，item5作为context下，item3即为正样本，然后类似的，negative sampling方式按照item频率分布，采样出的其他item即为负样本




## Word2Vec

- Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word

  ![](/home/leiwang/Markdown/推荐系统/v2-35339b4e3efc29326bad70728e2f469c_r.jpg)



- 将常见的单词组合（word pairs）或者词组作为单个“words”来处理。
- 对高频次单词进行抽样来减少训练样本的个数。
- 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。
- 对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。
- **负采样（negative sampling）**解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。
- 我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“
- 使用“一元模型分布（unigram distribution）”来选择“negative words”。
- 一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。



## A Survey

- collaborative filtering

- content-based recommender systems

- hybrid recommerder systems

- 深度学习网络

  - MLP
    - Neural Extension of Traditional Recommendation Methods.
      - Many existing recommendation models are essentially linear methods. MLP can be used to add nonlinear transformation to existing RS approaches and interpret them into neural extensions
      - NNMF:
      - NCF: Neural Collaborative Filtering:  扩展MF到神经网络中，对于显示反馈，利用加权的MSE损失训练，对于隐式反馈，利用二元交叉熵损失训练。Negative sampling方法可以用来减轻训练未观察数据的大小
      - DeepFM
      - xDeepFM
      - NFM
    - Feature Representation Learning with MLP
      - Wide and Deep:  The list of recommendations is generated based on the predicted scores.
      - YouTube DNN
      - Collaborative metric learning(CMF): replaces the dot product of MF with Euclidean
        distance
    - Recommendation with Deep Structured Semantic Model:
      - Deep Semantic Similarity-Based Personalized Recommendation
      - Multi-View Deep Neural Network (MV-DNN)
  - Autoencoder
    - using autoencoder to learn lower dimensional feature representations at the bottleneck layer
    - filling the blanks of the interaction matrix directly in the reconstruction layer
    - Autoencoder-Based Collaborative Filtering:
      - AutoRec [126] takes user partial vectors r (u ) or item partial vectors r (i ) as input and aims to reconstruct them in the output layer.
      - CFN:
      - CDAE
      - Multi-VAE
    - Feature Representation Learning with Autoencoder:
      - Collaborative deep learning:CDL
  - CNN
    - Feature Representation Learning with CNNs
    - CNNs can be used for feature representation learning from multiple sources such as image, text, audio, video, and more.
    - CNN-Based Collaborative Filtering
    - Graph CNNs for Recommendation
  - RNN
    - RNNs are suitable for sequential data processing. As such, RNNs become a natural choice for
      dealing with the temporal dynamics of interactions and sequential patterns of user behaviours, as well as side information with sequential signals
    - Session-Based Recommendation Without User Identifier.
      - GRU4Rec
  - RBM
  - Adversarial Network(AN)
  - Attention Models(AM)
  - Deep Reinforement Learning(DRL)

- RNN、CNN适合于序列数据

  ![](/home/leiwang/Markdown/论文/Screenshot from 2019-08-01 16-13-22.png)

![](/home/leiwang/Markdown/论文/Screenshot from 2019-08-01 16-14-01.png)

