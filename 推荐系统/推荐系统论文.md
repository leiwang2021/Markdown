# 推荐系统论文

## FM

- ![img](https://upload-images.jianshu.io/upload_images/4155986-990377c58bf6a215.png?imageMogr2/auto-orient/)

- 求解
  - 在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出
  - 为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。
  - ![img](https://upload-images.jianshu.io/upload_images/4155986-1f638fe25a63244c.png?imageMogr2/auto-orient/)
  - 那么ωij组成的矩阵可以表示为：	![img](https://upload-images.jianshu.io/upload_images/4155986-a262e2244174e776.png?imageMogr2/auto-orient/)

- n为特征维度，k为超参数，即共有n\*k个参数

  ![img](https://upload-images.jianshu.io/upload_images/4155986-6d08a2cdcc6668fb.png?imageMogr2/auto-orient/)

​	



## FFM

- 同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field
- 在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj
- 例如： “Day=26/11/15”这个特征与“Country”特征和“Ad_type"特征进行关联的时候使用不同的隐向量
- 假设样本的 n个特征属于 f个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型
- 如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk个
- ![img](https://upload-images.jianshu.io/upload_images/4155986-d04fed8047209d53.png?imageMogr2/auto-orient/)

- nf个向量，Xi与不同的Xj组合时，Xi部分所对应的特征向量是不同的，有f个，而在FM中是相同的，只有1个

## DeepFM

## 创新点

- 将特征转换成为one-hot的形式，但是将One-hot类型的特征输入到DNN中，会导致网络参数太多
- DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两部分**共享同样的输入**。
- CTR的输入一般是及其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。
- 1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2)在FM里得到的隐变量Vik现在作为了嵌入层网络的权重。
- weights['feature_embeddings'] 存放的每一个值其实就是FM中的vik，所以它是F * K的。其中，F代表feture的大小(将离散特征转换成one-hot之后的特征总量),K代表dense vector的大小。
- weights['feature_bias']是FM中的一次项的权重。

### Embedding

- 网络输入由三部分 组成  Xi_train:  N\*F     Xv_train:  N\*F    ,y_train:  N       F为field_size

- One-hot型的矩阵相乘，可以简化为查表操作，这大大降低了运算量。

- tf.nn.embedding_lookup

  由 feature_size\*K    以及   N\*F

  得到  N\*F\*K    ,即 embedding

  ```python
  Q = tf.reshape(tf.range(30), (10, 3))
  ne = tf.reshape(tf.range(8), (4, 2))
  item_emb = tf.nn.embedding_lookup(Q, ne)
  print(item_emb.shape)  
  a = tf.reshape(tf.range(8), [4, 2,1])
  print(a.shape)
  mu3 = tf.multiply(item_emb, a)
  print(mu3.shape)
  with tf.Session() as sess:
      print(sess.run(item_emb))
      print('---------')
      print(sess.run(a))
      print('---------')
      print(sess.run(mu3))  
      
  '''
  (4, 2, 3)
  (4, 2, 1)
  (4, 2, 3)
  
  [[[ 0  1  2]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 9 10 11]]
   [[12 13 14]
    [15 16 17]]
   [[18 19 20]
    [21 22 23]]]
  ---------
  [[[0]
    [1]]
   [[2]
    [3]]
   [[4]
    [5]]
   [[6]
    [7]]]
  ---------
  [[[  0   0   0]
    [  3   4   5]]
   [[ 12  14  16]
    [ 27  30  33]]
   [[ 48  52  56]
    [ 75  80  85]]
   [[108 114 120]
  '''
  ```

- tf.nn.dropout

  - tf.nn.dropout是TensorFlow里面为了防止或减轻过拟合而使用的函数，它**一般用在全连接层**。
  - Dropout就是在不同的训练过程中随机扔掉一部分神经元。也就是让某个神经元的激活值以一定的概率p，让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。

### size

- 第一层：   (feature_size, embedding_size)    ,如（256, 8）
- 第二层：   (field_size*embedding,   layer_size) ,  如（39\*8, 32）
- 第三层： （32，  32）
- 最后一层:   (field_size+embedding_size+32,  1),    如（39+8+32,   1）

## GBDT+LR

- 好的特征和好的模型很重要。Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale)

