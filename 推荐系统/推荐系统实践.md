# 评分预测问题

## 隐语义模型与矩阵分解模型

### 传统的SVD分解

- 首先用一个简单的方法补全稀疏评分矩阵
- 进行SVD分解
- 取最大的k个奇异值对应的行和列酉向量构成降维矩阵，重新获得评分矩阵
- 该方法计算复杂度太高

### Simon Funk的SVD分解(Latent Factor Model: LFM)

$$
\sum_{k=1}p_{u,k}q_{i,k}
$$



- 隐性反馈数据集：如何给用户生成负样本
  - 对每个用户，要保证正负样本的平衡
  - 采样负样本时，选取热门物品，而用户却没有行为的物品
  - 用户兴趣向量p和物品向量q
- 将评分矩阵分解为两个低维矩阵的乘积　　$R=P^TQ$

### 加入偏置项的LFM(BasicSVD)



### 考虑邻域影响的LFM(SVD++)



# 推荐系统的召回模型

## 协同过滤

### 基于用户的协同过滤

### 基于物品的协同过滤

### 基于模型的协同过滤(如矩阵分解)

## 向量化召回

- 主要通过模型来学习用户和物品的兴趣向量，并通过内积来计算用户和物品之间的相似性，从而得到最终的候选集。其中，比较经典的模型便是Youtube召回模型。在实际线上应用时，由于物品空间巨大，计算用户兴趣向量和所有物品兴趣向量的内积，耗时十分巨大，有时候会通过局部敏感Hash等方法来进行近似求解。

- 对于在线服务来说，有严格的性能要求，必须在几十毫秒内返回结果。因此，youtube没有重新跑一遍模型，而是通过保存用户兴趣embedding和视频兴趣embedding，通过最近邻搜索的方法得到top N的结果。该近似方法中的代表是局部敏感Hash方法

- 局部敏感哈希(Locality-Sensitive Hashing, LSH)

- 向量化召回是目前推荐召回核心发展的一代技术，但是它对模型结构做了很大的限制，必须要求模型围绕着用户和向量的embedding展开，同时在顶层进行内积运算得到相似性。在深度学习领域其实模型结构层出不穷，百花齐放，但是这样一个特定的结构实际上对模型能力造成了很大的限制。

## 深度树匹配

- 如果说向量化召回通过内积运算的方式打开了全库搜索的天花板，那么下一阶段应该是：能否设计一套全新的推荐算法框架，它允许容纳任意先进的模型而非限定内积形式，并且能够对全库进行更好的检索。**深度树匹配**，就是从这个视角出发做的技术探索。
- 协同过滤模型无法做到全局检索，而向量化模型对模型的结构进行了限制。深度树匹配模型解决了上述两个方面的限制，可以做到全局检索+使用先进模型。



## 基于深度学习(Item2Vec)



# 推荐系统的排序模型

## 浅层模型

- LR
- GBDT
- LR+GBDT

## 深层模型

- FM

- FFM

- DeepFFM

- Wide and Deep

  


# 冷启动

- 用户冷启动：即新来一个用户，如何做个性化推荐
- 物品冷启动: 即新的物品如何推荐给可能对它感兴趣的用户
- 系统冷启动: 即如何在一个新开发的网站（没有用户，没有用户行为，只有部分物品信息）上设计个性化推荐系统，从而在项目刚发布时就让用户体会到个性化推荐



# 基于Embedding的推荐系统召回策略

- 因为单个召回算法得到的结果一般都很难满足业务需求，所以通常都采取多路召回方式，如热门推荐、协同过滤、主题模型、内容召回以及模型召回等
- 排序阶段：对多个召回方法的结果进行统一打分并排序，选出最优Top K
- 步骤
  - 收集数据：神经网络需要大量的训练样本；
  - 数据处理：根据具体问题将数据按照embedding的场景标准进行处理；
    - Item2vec中把用户浏览的item集合等价于word2vec中的word的序列，即句子（忽略了商品序列空间信息spatial information） 。出现在同一个集合的*商品*对视为 positive
    - 用户点击过一篇doc，说明用户对doc产生了一定的兴趣，当我们把每个doc用实体词标签标记之后，就相当于用户对这些实体词感兴趣[user_id, keywords]，其中keywords是用“|”分隔的词的集合。当我们将用户与多篇doc关联起来之后，就可以得到用户与实体词的兴趣。最后可以使用[user_id, keyword, score]进行标记。
    - 通过数据处理之后可以得到user_keywords, user_index, keyword_index
    - 接下来，需要模拟一个分类或者回归的场景，用于学习embedding的weights。
  - 训练weights：建立embedding模型训练weights；
    - 注意，这里主要是模拟场景，并非做真的分类模型。训练模型的预测结果不是我们的最终目的，我们的目的是得到模型的parameters，即weights，所以，我们不需要真正的care模型结果是否精确。
    - Embedding的weights的作用:
      - 在embedding Space中找到某个样本点`top K`的最近邻；
      - 作为machine learning model的input
      - Visualization：低维空间可视化
    - 模拟正负样本：
      - 通过数据构造，我们得到了user与keyword的关系，知道每个user对keyword的喜好程度。为了构建分类模型，这里我们将已有的user与keyword的对应关系作为正样本，负样本通过人工的从user集合与keyword集合进行构造，当出现的user与keyword对不在已有的数据内，我们就将其作为负样本。
    - Embedding模型
      - Input: user与keyword同时作为输入；
      - Embedding: 每个user和keyword使用同样的embedding size；
      - Dot: 使用dot product合并embedding；
      - Reshape: 将点积Reshape为一个值；
      - Dense: 使用sigmoid激活函数处理output。
    - 提取user weight: 
      - 当模型训练完之后，我们可以通过下面的方法提取user的weight
      - user_weights是用户权重集合，每一行表示一个用户，相当于通过embedding训练user的向量表示，接下来可以进一步对用户进行可视化分析，同时可以通过计算相似度得到每个用户最相似的K个用户。
  - 使用weights：使用Embedding weight进行recommendation和visualizations.
    - TopK推荐:上面我们假设了每篇doc的keywords就是user对应的keywords，因此，我们可以直接通过计算weights 的cosine相似度进行推荐
    - 可以从相似的几个用户的user behavior里面，筛选出最近点击的或者最喜欢的doc给用户119。
    - 上面的方法主要是找到user的向量表示，类似于user-cf，通过表示向量我们可以计算出用户最相似的其它用户，进而进行推荐。下一步的推荐策略还可以进一步扩展：
      - 如何准确对用户推荐doc：可以采用user_similarity_score * user_doc_score，然后取top N；
      - 训练基于doc的embedding，对每篇doc进行推荐；
- 代码地址: <https://github.com/csuldw/MachineLearning/tree/master/Recommendation%20System>





  



